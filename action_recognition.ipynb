{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "action recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCBVv7y5vY_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvBLW0xo1QlX",
        "colab_type": "text"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qQuzKWVxEcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import sys\n",
        "import torch.nn as nn \n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "import scipy.io\n",
        "from torch.utils.data import Dataset\n",
        "from time import time\n",
        "from torch import nn, optim\n",
        "from torchvision.models import vgg16\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from torchvision import  datasets, transforms, models, get_image_backend\n",
        "from torchvision.transforms import Compose\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPqRQ04Z10JI",
        "colab_type": "text"
      },
      "source": [
        "## Optical Flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuORYo9W1VLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/sniklaus/pytorch-liteflownet\n",
        "!bash /content/pytorch-liteflownet/download.bash\n",
        "!pip install flowiz -U\n",
        "!pip install numpy tqdm matplotlib eel\n",
        "!apt install ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaqjLDDa1nE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/georgegach/flowiz.git\n",
        "import flowiz as fz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETmfBIUm1xXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_op_flow(frame1,frame2):\n",
        "    \n",
        "    imgc =np.array( cv2.imread('/content/sec.png', cv2.IMREAD_UNCHANGED))\n",
        "    imgc = np.array(cv2.imread('/content/sec.png', cv2.IMREAD_UNCHANGED))\n",
        "    width = 1024\n",
        "    height = 436 # keep original height\n",
        "    dim = (width, height)\n",
        "    resized1 = cv2.resize(frame1, dim, interpolation = cv2.INTER_AREA)\n",
        "    resized2 = cv2.resize(frame2, dim, interpolation = cv2.INTER_AREA)\n",
        "    count = 1\n",
        "    cv2.imwrite(\"frame%d.png\" % count, resized1)\n",
        "    count = 2\n",
        "    cv2.imwrite(\"frame%d.png\" % count, resized2)\n",
        "\n",
        "    !python /content/pytorch-liteflownet/run.py --model sintel --first /content/frame1.png --second /content/frame2.png --out ./outfull.flo\n",
        "    files = glob('/content/outfull.flo')\n",
        "    optical_frame_org = fz.convert_from_file(files[0])\n",
        "    \n",
        "    return optical_frame_org"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSIzRWQK4Hfl",
        "colab_type": "text"
      },
      "source": [
        "## Cropping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luwlt8gf2Gv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append (\"/content/drive/My Drive/crop\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ugrb2CzDK-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from crop import crop_dims\n",
        "\n",
        "def cropping(image,box):\n",
        "    \n",
        "    croped_img = image[ box[0][1]:box[0][3],box[0][0]:box[0][2]]\n",
        "    return croped_img\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4jEMjew2lWO",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOtFFhDv2naV",
        "colab_type": "text"
      },
      "source": [
        "## Download & preparing data dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvmdfPfQNGAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"/content/drive/My Drive/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv7lLczbUcE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(data_path):\n",
        "    os.mkdir(data_path)\n",
        "    !wget ftp://ftp.merl.com/pub/tmarks/MERL_Shopping_Dataset/Videos_MERL_Shopping_Dataset.zip\n",
        "    with zipfile.ZipFile(\"/content/Videos_MERL_Shopping_Dataset.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "    !wget ftp://ftp.merl.com/pub/tmarks/MERL_Shopping_Dataset/Labels_MERL_Shopping_Dataset.zip\n",
        "    with zipfile.ZipFile(\"/content/Labels_MERL_Shopping_Dataset.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KMb7C5549wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "videos_path = os.path.join(data_path,'/content/drive/My Drive/data/Videos_MERL_Shopping_Dataset')\n",
        "test_path = os.path.join(data_path,'test')\n",
        "valid_path = os.path.join(data_path,'valid')\n",
        "labels_path = os.path.join(data_path,'Labels_MERL_Shopping_Dataset')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsi2YxE534Xg",
        "colab_type": "text"
      },
      "source": [
        "## Transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKfYKUHz6vdS",
        "colab_type": "code",
        "outputId": "7c9c8a77-73e6-4385-eee0-c7aff087e04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "train_transforms = transforms.Compose([ transforms.ToPILImage(),\n",
        "                                        transforms.Scale((224,224)),\n",
        "                                        transforms.RandomRotation(10),\n",
        "                                        transforms.ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                      transforms.Scale((224,224)),\n",
        "                                      transforms.ToTensor()])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU-4PmH5G6pf",
        "colab_type": "text"
      },
      "source": [
        "## Generate Data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXuoQa5NG96z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def video_frames(path):\n",
        "    \n",
        "    vidcap = cv2.VideoCapture(path)\n",
        "    fv = []\n",
        "    success,image = vidcap.read()\n",
        "    fv.append(image)\n",
        "    i = 0\n",
        "    while success:\n",
        "        i+=1\n",
        "        success,image = vidcap.read()\n",
        "        fv.append(image)\n",
        "        print('\\r Frame no. {}'.format(i),end='')\n",
        "    fv = np.asarray(fv)\n",
        "    return fv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpLkbTmTidUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actions = [\"Reach To Shelf\",\"Retract From Shelf\",\"Hand In Shelf\",\"Inspect Product\",\"Inspect Shelf\"] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jVUN59VHG_f",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX1-jnJpHL0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_len(mat_file):\n",
        "    num_seq = 0\n",
        "    for f in mat_file:\n",
        "        for i,j in f[0]:\n",
        "            num_seq += int((j-i+1)/6.5)\n",
        "\n",
        "    return num_seq\n",
        "  \n",
        "def video_frames(path):\n",
        "    vidcap = cv2.VideoCapture(path)\n",
        "    fv = []\n",
        "    success,image = vidcap.read()\n",
        "    fv.append(image)\n",
        "    i = 0\n",
        "    while success:\n",
        "        i+=1\n",
        "        success,image = vidcap.read()\n",
        "        fv.append(image)\n",
        "        print('\\r Frame no. {}'.format(i),end='')\n",
        "    fv = np.asarray(fv)\n",
        "    return fv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k24J_O0PHSqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_loader(Dataset):\n",
        "    \n",
        "    def __init__(self,labels_path,vidoes_path,transform):\n",
        "        self.transform = transform\n",
        "        self.labels_path = labels_path\n",
        "        self.videos_path = vidoes_path\n",
        "        self.mat_files =[]\n",
        "        for file in os.listdir(self.labels_path):\n",
        "            if file[-4:] =='.mat':\n",
        "                self.mat_files.append(file)\n",
        "        self.file_indx = 0\n",
        "        self.video_frames = None\n",
        "        self.mat_file = None\n",
        "        self.class_num = 0\n",
        "        self.class_subset = 0\n",
        "        self.start = -1\n",
        "        self.end = -1\n",
        "        \n",
        "    def __len__(self):  \n",
        "        total = 0\n",
        "        for mat_file in self.mat_files :\n",
        "            file = scipy.io.loadmat(os.path.join(self.labels_path, mat_file))['tlabs']\n",
        "            total += data_len(file)\n",
        "        return total\n",
        "    def __getitem__(self,indx):\n",
        "        \n",
        "        if self.video_frames == None:\n",
        "            self.video_frames = video_frames(os.path.join(self.videos_path,self.mat_files[self.file_indx][:-9]+'crop.mp4'))\n",
        "          \n",
        "       \n",
        "        if self.mat_file == None:\n",
        "            self.mat_file = scipy.io.loadmat(os.path.join(self.labels_path, self.mat_files[self.file_indx]))['tlabs']\n",
        "        \n",
        "        if self.start == -1 or self.end == -1:\n",
        "#             print(str(os.path.join(self.labels_path, self.mat_files[self.file_indx]))+'\\t'+str(self.class_num)+'\\t'+str(self.class_subset))\n",
        "            self.start, self.end = self.mat_file[self.class_num][0][self.class_subset]\n",
        "            \n",
        "        if (int(self.start)-int(self.end)) >= 0:\n",
        "            self.start, self.end = -1, -1\n",
        "            self.class_subset +=1\n",
        "           \n",
        "            if self.class_subset >= len(self.mat_file[self.class_num][0]) -1:\n",
        "                self.class_subset = 0\n",
        "                self.class_num += 1\n",
        "                if self.class_num > len(self.mat_file)-1:\n",
        "                    self.class_num = 0\n",
        "                    del self.video_frames\n",
        "                    self.video_frames = None\n",
        "                    self.mat_file = None\n",
        "                    self.file_indx +=1\n",
        "        \n",
        "        if self.video_frames == None:\n",
        "#             print(os.path.join(self.videos_path,self.mat_files[self.file_indx][:-9]+'crop.mp4'))\n",
        "            self.video_frames = video_frames(os.path.join(self.videos_path,self.mat_files[self.file_indx][:-9]+'crop.mp4'))\n",
        "         \n",
        "        \n",
        "        if self.mat_file == None:\n",
        "            self.mat_file = scipy.io.loadmat(os.path.join(self.labels_path, self.mat_files[self.file_indx]))['tlabs']\n",
        "        if self.start == -1 or self.end == -1:\n",
        "            print(str(os.path.join(self.labels_path, self.mat_files[self.file_indx]))+'\\t'+str(self.class_num)+'\\t'+str(self.class_subset))\n",
        "            self.start, self.end = self.mat_file[self.class_num][0][self.class_subset]\n",
        "  \n",
        "        seq = []\n",
        "        for i in range(6):\n",
        "            ###Code###  \n",
        "            frame = self.video_frames[self.start]\n",
        "            cropping_dim = crop_dims(frame)\n",
        "            cropped_img = cropping(frame, cropping_dim)\n",
        "            \n",
        "            frame_aux = self.video_frames[max(self.start-5,0)]\n",
        "            \n",
        "            optical_frame_org =  get_op_flow(frame,frame_aux) \n",
        "            cropped_opt_img = cropping(optical_frame_org, cropping_dim)\n",
        "\n",
        "            \n",
        "            frame = self.transform(frame)\n",
        "            cropped_img = self.transform(cropped_img)\n",
        "            optical_frame_org = self.transform(optical_frame_org)\n",
        "            cropped_opt_img = self.transform(cropped_opt_img)\n",
        "            \n",
        "            imgs = torch.stack([optical_frame_org,cropped_opt_img,cropped_img,frame])\n",
        "            seq.append(imgs)\n",
        "\n",
        "            self.start +=1\n",
        "        seq = torch.stack(seq)\n",
        "        return seq, np.ones(6)*self.class_num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqFuHMfPHYzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = Data_loader(labels_path,\n",
        "                         videos_path,train_transforms)\n",
        "valid_data = Data_loader(valid_path,\n",
        "                         videos_path,train_transforms)\n",
        "test_data = Data_loader(test_path,\n",
        "                         videos_path,test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjSOcEQuzals",
        "colab_type": "text"
      },
      "source": [
        "# Displayers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUuBWhLazaPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    \n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HET2cnU6-Pyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_graph(train_losses, valid_losses, train_accs, valid_accs):\n",
        "    plt.plot(train_losses, label='Training loss')\n",
        "    plt.plot(valid_losses, label='Validation loss')\n",
        "    plt.plot(train_accs, label='Train accuracy')\n",
        "    plt.plot(valid_accs, label='Validation accuracy')\n",
        "    plt.legend(frameon=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BuP4CKpzhTc",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwTpb0CGHf4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, latent_dim,num_projection_img):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        vgg = vgg16(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(vgg.classifier[0].in_features, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
        "        )\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(vgg.classifier[0].in_features*num_projection_img, latent_dim, nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
        "        ))\n",
        "    def forward(self,x):\n",
        "        batch, _, c, h, w = x.shape \n",
        "        x = x.view(-1, c,h,w)\n",
        "        x=self.feature_extractor(x)\n",
        "        x = x.view(batch,-1)        \n",
        "        x=self.final(x)\n",
        "        return x\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI-wGHaoH451",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.latent_attention = nn.Linear(latent_dim, attention_dim)\n",
        "        self.hidden_attention = nn.Linear(hidden_dim, attention_dim)\n",
        "        self.joint_attention = nn.Linear(attention_dim, 1)\n",
        "\n",
        "    def forward(self, latent_repr, hidden_repr):\n",
        "        if hidden_repr is None:\n",
        "            hidden_repr = [\n",
        "                Variable(\n",
        "                    torch.zeros(latent_repr.size(0), 1, self.hidden_attention.in_features), requires_grad=False\n",
        "                ).float()\n",
        "            ]\n",
        "        h_t = hidden_repr[0]\n",
        "        latent_att = self.latent_attention(latent_att)\n",
        "        hidden_att = self.hidden_attention(h_t)\n",
        "        joint_att = self.joint_attention(F.relu(latent_att + hidden_att)).squeeze(-1)\n",
        "        attention_w = F.softmax(joint_att, dim=-1)\n",
        "        return attention_w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU6PMlXJH8E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, num_classes, num_projection_img=4, latent_dim=512, lstm_layers=1, hidden_dim=1024, bidirectional=False, attention=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "        self.latent_dim=latent_dim\n",
        "        self.encoder = Encoder(latent_dim,num_projection_img)\n",
        "        self.lstm = nn.LSTM(latent_dim, hidden_dim,lstm_layers,bidirectional)\n",
        "\n",
        "        self.output_layers = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim, momentum=0.01),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "        self.attention = attention\n",
        "        self.attention_layer = nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        batch_size, seq_length, num_projection_img, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length , num_projection_img, c, h, w)\n",
        "        x = self.encoder(x)\n",
        "        \n",
        "        x = x.view(batch_size, seq_length, -1)\n",
        "        x,_ = self.lstm(x)\n",
        "        \n",
        "        if self.attention:\n",
        "            self.attention_layer(x).squeeze(-1)\n",
        "            attention_w = F.softmax(self.attention_layer(x).squeeze(-1), dim=-1)\n",
        "            x = torch.sum(attention_w.unsqueeze(-1) * x, dim=1)\n",
        "        x = x.view(batch_size*seq_length,-1)\n",
        "        x = self.output_layers(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGhOngAoH_-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ConvLSTM(num_classes=5, latent_dim=512)\n",
        "model.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aULgfPuldfjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('/content/drive/My Drive/Models'):\n",
        "    os.mkdir('/content/drive/My Drive/Models')\n",
        "model_file = \"/content/drive/My Drive/Models/action_recognation.pth\"\n",
        "back_up_model_file = \"/content/drive/My Drive/Models/action_recognation_backup.pth\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFpT37Q0IYlp",
        "colab_type": "text"
      },
      "source": [
        "# Workers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paGdKi4JIVCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, valid_loader, n_epochs, model_file, back_up_model_file):\n",
        "    \n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    min_valid_loss = np.Inf\n",
        "    \n",
        "    for e in range(1,n_epochs+1):\n",
        "  \n",
        "        epoch_start = time()\n",
        "        batch_number = 0\n",
        "\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        batch_start = time()\n",
        "        model.train()   \n",
        "        for x,y in train_loader:\n",
        "            batch_number += 1\n",
        "           \n",
        "            if torch.cuda.is_available() :\n",
        "                x, y = x.cuda(), y.cuda()\n",
        "            y =  y.long()\n",
        "            batchsize, seq,img_no,c,h,w = x.shape\n",
        "            optimizer.zero_grad()\n",
        "            y_ = model(x) \n",
        "\n",
        "            y = y.view(-1,1)\n",
        "            y = y.squeeze()\n",
        "            y_ = y_.view(batchsize*seq,-1)\n",
        "            loss = criterion(y_,y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            ps = torch.exp(y_)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == y.view(*top_class.shape)\n",
        "            train_acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "            delay = time()-batch_start\n",
        "            \n",
        "            print(\"\\rbatch size : {}  batch per total no of batches : {}/{} \\\n",
        "            \\ntrain batch finished : {:.3f} % \\ntime left : {}s \\ndelay : {}s \\\n",
        "            \\nloss : {}\\n\\n\".format(len(x), batch_number, len(train_loader),\n",
        "            batch_number/len(train_loader) *100., delay * (len(train_loader)-\\\n",
        "            batch_number), delay, loss.item()))\n",
        "            \n",
        "            batch_start = time()\n",
        "            torch.save(model.state_dict(), back_up_model_file)\n",
        "\n",
        "            \n",
        "            \n",
        "        valid_loss = 0\n",
        "        valid_acc = 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_number = 0\n",
        "            batch_start = time()\n",
        "            acc = 0\n",
        "            for x,y in valid_loader:\n",
        "                batch_number += 1\n",
        "                if torch.cuda.is_available() :\n",
        "                    x, y = x.cuda(), y.cuda()\n",
        "                y =  y.long()\n",
        "                batchsize, seq,img_no,c,h,w = x.shape\n",
        "\n",
        "                y_ = model(x)       \n",
        "                y = y.view(-1,1)\n",
        "                y = y.squeeze()\n",
        "                y_ = y_.view(batchsize*seq,-1)\n",
        "                loss = criterion(y_,y)\n",
        "                valid_loss += loss.item()\n",
        "                \n",
        "               \n",
        "                ps = torch.exp(y_)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == y.view(*top_class.shape)\n",
        "                valid_acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "                delay = time()-batch_start\n",
        "      \n",
        "                print(\"batch size : {}\\nbatch per total no of batches : {}/{} \\\n",
        "                \\ntrain batch finished : {:.3f} % \\ntime lift : {} s\\\n",
        "                \\ndelay : {}s \\nloss : {}\\n\\n\".format(len(x),\n",
        "                batch_number, len(valid_loader), batch_number/len(valid_loader)\\\n",
        "                      *100., delay * (len(valid_loader)-batch_number), delay,\\\n",
        "                                                      loss.item()))\n",
        "                batch_start = time()\n",
        "                \n",
        "\n",
        "        train_loss /= len(train_loader)     \n",
        "        train_acc /= len(train_loader)  \n",
        "\n",
        "        valid_loss /= len(valid_loader)\n",
        "        valid_acc /= len(valid_loader)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accs.append(valid_acc)\n",
        "\n",
        "        if min_valid_loss > valid_loss:\n",
        "            print ('Validation loss decreased ({:.6f} --> {:.6f}). \\\n",
        "            Saving model ...\\n'.format(min_valid_loss, valid_loss))\n",
        "            min_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), model_file)\n",
        "        \n",
        "        delay = time() - epoch_start\n",
        "\n",
        "\n",
        "        print(\"Epoch : {} \\nTrain Finished : {:.3f} %\\nTime Left : {:.3f} s\\\n",
        "        \\nTraining Loss : {:.6f} \\nValidation Loss : {:.6f} \\nTrain Accuracy :\\\n",
        "        {:.3f} %\\nValidation Accuracy : {:.6f} %\\nDelay : {:.6f} s \\n\\n\".format(\n",
        "            e,e / n_epochs * 100., delay * (n_epochs - e) ,train_loss, valid_loss\n",
        "            ,train_acc * 100.,valid_acc * 100.,delay))\n",
        "        \n",
        "    return train_losses, train_accs, valid_losses, valid_accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW-AfficC5lo",
        "colab_type": "text"
      },
      "source": [
        "#Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3zRWQuakbQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data,batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_9P2qeKQDgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses, train_accs, valid_losses, valid_accs = train(model, train_loader, valid_loader,100,model_file,back_up_model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}